{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bb4ff46-89c2-4f88-a03d-a0c3e47de585",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T10:43:00.939174Z",
     "iopub.status.busy": "2024-09-10T10:43:00.938927Z",
     "iopub.status.idle": "2024-09-10T10:43:16.215004Z",
     "shell.execute_reply": "2024-09-10T10:43:16.214251Z",
     "shell.execute_reply.started": "2024-09-10T10:43:00.939149Z"
    }
   },
   "source": [
    "## Write to Iceberg without upserting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb47db53-a8df-4473-b46c-dcbc0ad6e59d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T10:24:52.897208Z",
     "iopub.status.busy": "2024-09-11T10:24:52.896863Z",
     "iopub.status.idle": "2024-09-11T10:28:56.653233Z",
     "shell.execute_reply": "2024-09-11T10:28:56.652168Z",
     "shell.execute_reply.started": "2024-09-11T10:24:52.897175Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1084cfff1f9747cd8d37eed31084e4a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody><tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>48</td><td>application_1726023487946_0049</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-3-201.eu-central-1.compute.internal:20888/proxy/application_1726023487946_0049/\" class=\"emr-proxy-link j-2WV3LZUNFP6DB application_1726023487946_0049\" emr-resource=\"j-2WV3LZUNFP6DB\n",
       "\" application-id=\"application_1726023487946_0049\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-3-238.eu-central-1.compute.internal:8042/node/containerlogs/container_1726023487946_0049_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></tbody></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dca5d46b257e4b8ea553d41b8665f052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, TimestampType, LongType\n",
    "\n",
    "# Define the updated schema\n",
    "schema = StructType([\n",
    "    StructField(\"Campaign_ID\", StringType(), True),\n",
    "    StructField(\"DataEntries\", ArrayType(StructType([\n",
    "        StructField(\"DataName\", StringType(), True),\n",
    "        StructField(\"DataType\", StringType(), True),\n",
    "        StructField(\"DataValue\", StringType(), True),\n",
    "        StructField(\"TimeStamp\", StringType(), True)\n",
    "    ])), True),\n",
    "    StructField(\"VinNumber\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Kafka parameters\n",
    "kafka_bootstrap_servers = \"b-2.kafkaclustericeberg.wa9le5.c3.kafka.eu-central-1.amazonaws.com:9092,b-1.kafkaclustericeberg.wa9le5.c3.kafka.eu-central-1.amazonaws.com:9092\"\n",
    "kafka_topic = \"topic6\"\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaToIceberg\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.demo\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.demo.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.demo.warehouse\", \"s3://aws-emr-studio-381492251123-eu-central-1/ICEBERG/\") \\\n",
    "    .config(\"spark.sql.catalog.demo.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n",
    "    .config(\"spark.sql.streaming.checkpointLocation\", \"s3://aws-emr-studio-381492251123-eu-central-1/stream_checkpoint/\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create a DataFrame representing the stream of input lines from Kafka\n",
    "kafka_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Convert the value column from Kafka to a string and parse JSON\n",
    "parsed_df = kafka_df.select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\"))\n",
    "\n",
    "# Flatten the structure for easier querying\n",
    "flattened_df = parsed_df.select(\n",
    "    col(\"data.VinNumber\"),\n",
    "    explode(col(\"data.DataEntries\")).alias(\"entry\")\n",
    ").select(\n",
    "    col(\"VinNumber\"),\n",
    "    col(\"entry.DataName\").alias(\"Parameter_Name\"),\n",
    "    col(\"entry.DataType\"),\n",
    "    col(\"entry.DataValue\"),\n",
    "    # Convert the TimeStamp from string to timestamp\n",
    "    to_timestamp(from_unixtime(col(\"entry.TimeStamp\").cast(\"long\") / 1000)).alias(\"TimeStamp\")\n",
    ")\n",
    "\n",
    "# Add a watermark on the TimeStamp column\n",
    "watermarked_df = flattened_df.withWatermark(\"TimeStamp\", \"10 minutes\")\n",
    "\n",
    "# Perform the aggregation\n",
    "aggregated_df = watermarked_df.groupBy(\n",
    "    window(col(\"TimeStamp\"), \"30 minutes\"),\n",
    "    col(\"VinNumber\"),\n",
    "    col(\"Parameter_Name\")\n",
    ").agg(count(\"*\").alias(\"Count\"))\n",
    "\n",
    "# Adjust the DataFrame schema to match the Iceberg table schema\n",
    "result_df = aggregated_df \\\n",
    "    .select(\n",
    "        col(\"VinNumber\").cast(StringType()).alias(\"VinNumber\"),\n",
    "        to_timestamp(col(\"window.start\")).alias(\"Window_Start\"),\n",
    "        to_timestamp(col(\"window.end\")).alias(\"Window_End\"),\n",
    "        col(\"Parameter_Name\").cast(StringType()).alias(\"Parameter_Name\"),\n",
    "        col(\"Count\").cast(LongType()).alias(\"Count\")\n",
    "    )\n",
    "\n",
    "# Define the foreachBatch function\n",
    "def process_batch(batch_df, batch_id):\n",
    "    try:\n",
    "        # Show a sample of the batch data\n",
    "        batch_df.show(5, truncate=False)\n",
    "        \n",
    "        # Write the batch to Iceberg table\n",
    "        batch_df.write \\\n",
    "            .format(\"iceberg\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .save(\"demo.uraues_db.events_logs_agg6\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch {batch_id}: {str(e)}\")\n",
    "\n",
    "# Write the aggregated data to the Iceberg table using foreachBatch\n",
    "query = result_df.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .start()\n",
    "\n",
    "# Wait for the streaming query to terminate\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a503acf2-4966-4f4c-911c-44504c9e6832",
   "metadata": {},
   "source": [
    "## Writing SEVs without Upsert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22eead58-c316-4e92-966a-1f874f265766",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T11:31:34.945438Z",
     "iopub.status.busy": "2024-09-11T11:31:34.945145Z",
     "iopub.status.idle": "2024-09-11T11:33:45.477215Z",
     "shell.execute_reply": "2024-09-11T11:33:45.476336Z",
     "shell.execute_reply.started": "2024-09-11T11:31:34.945412Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cc7e74c61574787ac6b5bb617d30435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n<tbody><tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>55</td><td>application_1726023487946_0056</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-3-201.eu-central-1.compute.internal:20888/proxy/application_1726023487946_0056/\" class=\"emr-proxy-link j-2WV3LZUNFP6DB application_1726023487946_0056\" emr-resource=\"j-2WV3LZUNFP6DB\n\" application-id=\"application_1726023487946_0056\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-3-238.eu-central-1.compute.internal:8042/node/containerlogs/container_1726023487946_0056_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></tbody></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a227bc8078545fa992dcf446d976c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, TimestampType, LongType\n",
    "\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaToIceberg\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.demo\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.demo.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.demo.warehouse\", \"s3://aws-emr-studio-381492251123-eu-central-1/ICEBERG/\") \\\n",
    "    .config(\"spark.sql.catalog.demo.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n",
    "    .config(\"spark.sql.streaming.checkpointLocation\", \"s3://aws-emr-studio-381492251123-eu-central-1/stream_checkpoint/\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the schema\n",
    "ioc_parameters_schema = StructType([\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"InternalParameter\", StructType([\n",
    "        StructField(\"ParameterName\", StringType(), True),\n",
    "        StructField(\"ParamterType\", StringType(), True),\n",
    "        StructField(\"ParameterValue\", StringType(), True),\n",
    "        StructField(\"ParameterUnit\", StringType(), True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"SEV_ID\", StringType(), True),\n",
    "    StructField(\"VinNumber\", StringType(), True),\n",
    "    StructField(\"Timestamp\", TimestampType(), True),\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Severity\", StringType(), True),\n",
    "    StructField(\"SEV_Msg\", StringType(), True),\n",
    "    StructField(\"Origin\", StringType(), True),\n",
    "    StructField(\"NetworkType\", StringType(), True),\n",
    "    StructField(\"NetworkID\", StringType(), True),\n",
    "    StructField(\"IoC\", StructType([\n",
    "        StructField(\"Parameters\", ArrayType(ioc_parameters_schema), True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "# Kafka parameters\n",
    "kafka_bootstrap_servers = \"b-2.kafkaclustericeberg.wa9le5.c3.kafka.eu-central-1.amazonaws.com:9092,b-1.kafkaclustericeberg.wa9le5.c3.kafka.eu-central-1.amazonaws.com:9092\"\n",
    "kafka_topic = \"sev_topic2\"\n",
    "\n",
    "# Create a DataFrame representing the stream of input lines from Kafka\n",
    "kafka_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Convert the value column from Kafka to a string and parse the JSON data\n",
    "parsed_df = kafka_df.select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "# Flatten the structure for easier querying\n",
    "flattened_df = parsed_df.select(\n",
    "    col(\"VinNumber\"),\n",
    "    col(\"SEV_ID\"),\n",
    "    # Convert the TimeStamp from string to timestamp\n",
    "    to_timestamp(from_unixtime(col(\"TimeStamp\").cast(\"long\") / 1000)).alias(\"TimeStamp\")\n",
    ")\n",
    "\n",
    "# Add a watermark on the TimeStamp column\n",
    "watermarked_df = flattened_df.withWatermark(\"TimeStamp\", \"10 minutes\")\n",
    "\n",
    "# Perform hourly aggregation\n",
    "aggregated_df = watermarked_df \\\n",
    "    .withWatermark(\"Timestamp\", \"1 hour\") \\\n",
    "    .groupBy(\n",
    "        window(\"Timestamp\", \"1 hour\"),\n",
    "        \"VinNumber\"\n",
    "    ) \\\n",
    "    .agg(count(\"SEV_ID\").alias(\"SEV_Count\"))\n",
    "\n",
    "# Function to write data to Iceberg table\n",
    "def write_to_iceberg(batch_df, batch_id):\n",
    "    # Rename the window start and end columns\n",
    "    batch_df = batch_df.select(\n",
    "        col(\"window.start\").alias(\"WindowStart\"),\n",
    "        col(\"window.end\").alias(\"WindowEnd\"),\n",
    "        \"VinNumber\",\n",
    "        \"SEV_Count\"\n",
    "    )\n",
    "\n",
    "    # Write to Iceberg table\n",
    "    batch_df.write \\\n",
    "        .format(\"iceberg\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .saveAsTable(\"demo.uraues_db.sevs_agg2\")\n",
    "\n",
    "# Write the aggregated data to Iceberg\n",
    "query = aggregated_df.writeStream \\\n",
    "    .foreachBatch(write_to_iceberg) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start()\n",
    "\n",
    "# Wait for the streaming query to terminate\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00058c32-2bbf-4aba-82ac-08f4b18bb2c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T06:42:38.624485Z",
     "iopub.status.busy": "2024-09-12T06:42:38.624257Z",
     "iopub.status.idle": "2024-09-12T06:42:43.606773Z",
     "shell.execute_reply": "2024-09-12T06:42:43.605348Z",
     "shell.execute_reply.started": "2024-09-12T06:42:38.624459Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "521504fb7ddb492b9bfcea0dc4f56360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread cell_monitor-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/notebook-env/lib/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/mnt/notebook-env/lib/python3.9/threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/mnt/notebook-env/lib/python3.9/site-packages/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in cell_monitor\n",
      "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
      "  File \"/mnt/notebook-env/lib/python3.9/site-packages/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in <listcomp>\n",
      "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
      "KeyError: 'jobGroup'\n",
      "Interrupted by user\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540e8deb-64dc-44fe-b3ec-e174c5a936a8",
   "metadata": {},
   "source": [
    "## Upsert SEVS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd8a12a8-a7bd-48be-9f03-7053295b050b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T07:05:20.478518Z",
     "iopub.status.busy": "2024-09-12T07:05:20.478213Z",
     "iopub.status.idle": "2024-09-12T07:18:59.637026Z",
     "shell.execute_reply": "2024-09-12T07:18:59.635983Z",
     "shell.execute_reply.started": "2024-09-12T07:05:20.478492Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd661654c01b4963a892127c20fad3b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n<tbody><tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>6</td><td>application_1726118155659_0010</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-3-57.eu-central-1.compute.internal:20888/proxy/application_1726118155659_0010/\" class=\"emr-proxy-link j-19DNHL8WERNU6 application_1726118155659_0010\" emr-resource=\"j-19DNHL8WERNU6\n\" application-id=\"application_1726118155659_0010\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-3-117.eu-central-1.compute.internal:8042/node/containerlogs/container_1726118155659_0010_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></tbody></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c54b19aaea6340e9a6601b64981c9cbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, TimestampType\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaToIceberg\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.demo\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.demo.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.demo.warehouse\", \"s3://aws-emr-studio-381492251123-eu-central-1/ICEBERG/\") \\\n",
    "    .config(\"spark.sql.catalog.demo.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n",
    "    .config(\"spark.sql.streaming.checkpointLocation\", \"s3://aws-emr-studio-381492251123-eu-central-1/stream_checkpoint/\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the schema\n",
    "ioc_parameters_schema = StructType([\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"InternalParameter\", StructType([\n",
    "        StructField(\"ParameterName\", StringType(), True),\n",
    "        StructField(\"ParamterType\", StringType(), True),\n",
    "        StructField(\"ParameterValue\", StringType(), True),\n",
    "        StructField(\"ParameterUnit\", StringType(), True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"SEV_ID\", StringType(), True),\n",
    "    StructField(\"VinNumber\", StringType(), True),\n",
    "    StructField(\"Timestamp\", TimestampType(), True),\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Severity\", StringType(), True),\n",
    "    StructField(\"SEV_Msg\", StringType(), True),\n",
    "    StructField(\"Origin\", StringType(), True),\n",
    "    StructField(\"NetworkType\", StringType(), True),\n",
    "    StructField(\"NetworkID\", StringType(), True),\n",
    "    StructField(\"IoC\", StructType([\n",
    "        StructField(\"Parameters\", ArrayType(ioc_parameters_schema), True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "# Kafka parameters\n",
    "kafka_bootstrap_servers = \"b-1.kafkaprivatecluster.2vrw32.c3.kafka.eu-central-1.amazonaws.com:9092,b-2.kafkaprivatecluster.2vrw32.c3.kafka.eu-central-1.amazonaws.com:9092\"\n",
    "kafka_topic = \"sevs_topic2\"\n",
    "\n",
    "# Create a DataFrame representing the stream of input lines from Kafka\n",
    "kafka_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Convert the value column from Kafka to a string and parse the JSON data\n",
    "parsed_df = kafka_df.select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "# Flatten the structure for easier querying\n",
    "flattened_df = parsed_df.select(\n",
    "    col(\"VinNumber\"),\n",
    "    col(\"SEV_ID\"),\n",
    "    # Convert the TimeStamp from string to timestamp\n",
    "    to_timestamp(from_unixtime(col(\"TimeStamp\").cast(\"long\"))).alias(\"TimeStamp\")\n",
    ")\n",
    "\n",
    "# Add a watermark on the TimeStamp column\n",
    "watermarked_df = flattened_df.withWatermark(\"TimeStamp\", \"10 minutes\")\n",
    "\n",
    "# Perform hourly aggregation\n",
    "aggregated_df = watermarked_df \\\n",
    "    .withWatermark(\"Timestamp\", \"1 hour\") \\\n",
    "    .groupBy(\n",
    "        window(\"Timestamp\", \"1 hour\"),\n",
    "        \"VinNumber\"\n",
    "    ) \\\n",
    "    .agg(count(\"SEV_ID\").alias(\"SEV_Count\"))\n",
    "\n",
    "def write_to_iceberg(batch_df, batch_id):\n",
    "    try:\n",
    "        print(f\"Processing batch {batch_id}\")\n",
    "        print(f\"Batch size: {batch_df.count()} records\")\n",
    "        batch_df.show(5, truncate=False)\n",
    "\n",
    "        # Rename the window start and end columns\n",
    "        batch_df = batch_df.select(\n",
    "            col(\"window.start\").alias(\"WindowStart\"),\n",
    "            col(\"window.end\").alias(\"WindowEnd\"),\n",
    "            \"VinNumber\",\n",
    "            \"SEV_Count\"\n",
    "        )\n",
    "\n",
    "        # Read the current Iceberg table\n",
    "        current_table = spark.table(\"demo.uraues_db.sevs_agg2\")\n",
    "\n",
    "        # Perform the merge operation using DataFrame APIs\n",
    "        merged_df = current_table.alias(\"target\").join(\n",
    "            batch_df.alias(\"source\"),\n",
    "            (col(\"target.WindowStart\") == col(\"source.WindowStart\")) &\n",
    "            (col(\"target.WindowEnd\") == col(\"source.WindowEnd\")) &\n",
    "            (col(\"target.VinNumber\") == col(\"source.VinNumber\")),\n",
    "            \"full_outer\"\n",
    "        ).select(\n",
    "            coalesce(col(\"source.WindowStart\"), col(\"target.WindowStart\")).alias(\"WindowStart\"),\n",
    "            coalesce(col(\"source.WindowEnd\"), col(\"target.WindowEnd\")).alias(\"WindowEnd\"),\n",
    "            coalesce(col(\"source.VinNumber\"), col(\"target.VinNumber\")).alias(\"VinNumber\"),\n",
    "            when(col(\"source.SEV_Count\").isNotNull() & col(\"target.SEV_Count\").isNotNull(),\n",
    "                 greatest(col(\"source.SEV_Count\"), col(\"target.SEV_Count\")))\n",
    "            .otherwise(coalesce(col(\"source.SEV_Count\"), col(\"target.SEV_Count\")))\n",
    "            .alias(\"SEV_Count\")\n",
    "        )\n",
    "\n",
    "        # Log the merge results\n",
    "        print(\"Merged DataFrame:\")\n",
    "        merged_df.show(10, truncate=False)\n",
    "        print(f\"Merged DataFrame count: {merged_df.count()}\")\n",
    "\n",
    "        # Write the merged data back to the Iceberg table\n",
    "        merged_df.write \\\n",
    "            .format(\"iceberg\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .saveAsTable(\"demo.uraues_db.sevs_agg2\")\n",
    "\n",
    "        print(f\"Batch {batch_id}: Merged {batch_df.count()} records into the Iceberg table.\")\n",
    "\n",
    "        # Verify the write operation\n",
    "        updated_table = spark.table(\"demo.uraues_db.sevs_agg2\")\n",
    "        print(\"Updated Iceberg table:\")\n",
    "        updated_table.show(10, truncate=False)\n",
    "        print(f\"Updated Iceberg table count: {updated_table.count()}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch {batch_id}: {str(e)}\")\n",
    "\n",
    "# Write the aggregated data to Iceberg\n",
    "query = aggregated_df.writeStream \\\n",
    "    .foreachBatch(write_to_iceberg) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .start()\n",
    "\n",
    "# Wait for the streaming query to terminate\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0f90b7-e490-493f-bfb0-4bb320102cf3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Upsert Log Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd1a325-ce85-4b7c-8351-324de8870e3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T14:50:36.731205Z",
     "iopub.status.busy": "2024-09-11T14:50:36.730727Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a796f2ac4c034836945cacd34ee77d60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody><tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>64</td><td>application_1726023487946_0065</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-3-201.eu-central-1.compute.internal:20888/proxy/application_1726023487946_0065/\" class=\"emr-proxy-link j-2WV3LZUNFP6DB application_1726023487946_0065\" emr-resource=\"j-2WV3LZUNFP6DB\n",
       "\" application-id=\"application_1726023487946_0065\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-3-238.eu-central-1.compute.internal:8042/node/containerlogs/container_1726023487946_0065_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></tbody></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "276d2a6c7656488cbe7bef7259465fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, TimestampType, LongType\n",
    "\n",
    "# Define the updated schema\n",
    "schema = StructType([\n",
    "    StructField(\"Campaign_ID\", StringType(), True),\n",
    "    StructField(\"DataEntries\", ArrayType(StructType([\n",
    "        StructField(\"DataName\", StringType(), True),\n",
    "        StructField(\"DataType\", StringType(), True),\n",
    "        StructField(\"DataValue\", StringType(), True),\n",
    "        StructField(\"TimeStamp\", StringType(), True)\n",
    "    ])), True),\n",
    "    StructField(\"VinNumber\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Kafka parameters\n",
    "kafka_bootstrap_servers = \"b-2.kafkaclustericeberg.wa9le5.c3.kafka.eu-central-1.amazonaws.com:9092,b-1.kafkaclustericeberg.wa9le5.c3.kafka.eu-central-1.amazonaws.com:9092\"\n",
    "kafka_topic = \"topic8\"\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaToIceberg\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.demo\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.demo.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.demo.warehouse\", \"s3://aws-emr-studio-381492251123-eu-central-1/ICEBERG/\") \\\n",
    "    .config(\"spark.sql.catalog.demo.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n",
    "    .config(\"spark.sql.streaming.checkpointLocation\", \"s3://aws-emr-studio-381492251123-eu-central-1/stream_checkpoint/\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create a DataFrame representing the stream of input lines from Kafka\n",
    "kafka_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Convert the value column from Kafka to a string and parse JSON\n",
    "parsed_df = kafka_df.select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\"))\n",
    "\n",
    "# Flatten the structure for easier querying\n",
    "flattened_df = parsed_df.select(\n",
    "    col(\"data.VinNumber\"),\n",
    "    explode(col(\"data.DataEntries\")).alias(\"entry\")\n",
    ").select(\n",
    "    col(\"VinNumber\"),\n",
    "    col(\"entry.DataName\").alias(\"Parameter_Name\"),\n",
    "    col(\"entry.DataType\"),\n",
    "    col(\"entry.DataValue\"),\n",
    "    # Convert the TimeStamp from string to timestamp\n",
    "    to_timestamp(from_unixtime(col(\"entry.TimeStamp\").cast(\"long\") / 1000)).alias(\"TimeStamp\")\n",
    ")\n",
    "\n",
    "# Add a watermark on the TimeStamp column\n",
    "watermarked_df = flattened_df.withWatermark(\"TimeStamp\", \"10 minutes\")\n",
    "\n",
    "# Perform the aggregation\n",
    "aggregated_df = watermarked_df.groupBy(\n",
    "    window(col(\"TimeStamp\"), \"30 minutes\"),\n",
    "    col(\"VinNumber\"),\n",
    "    col(\"Parameter_Name\")\n",
    ").agg(count(\"*\").alias(\"Count\"))\n",
    "\n",
    "# Adjust the DataFrame schema to match the Iceberg table schema\n",
    "result_df = aggregated_df \\\n",
    "    .select(\n",
    "        col(\"VinNumber\").cast(StringType()).alias(\"VinNumber\"),\n",
    "        to_timestamp(col(\"window.start\")).alias(\"Window_Start\"),\n",
    "        to_timestamp(col(\"window.end\")).alias(\"Window_End\"),\n",
    "        col(\"Parameter_Name\").cast(StringType()).alias(\"Parameter_Name\"),\n",
    "        col(\"Count\").cast(LongType()).alias(\"Count\")\n",
    "    )\n",
    "\n",
    "# Define the foreachBatch function with merge logic\n",
    "def process_batch(batch_df, batch_id):\n",
    "    try:\n",
    "        print(f\"Processing batch {batch_id}\")\n",
    "        print(f\"Batch size: {batch_df.count()} records\")\n",
    "        batch_df.show(5, truncate=False)\n",
    "        \n",
    "        # Read the current Iceberg table\n",
    "        current_table = spark.table(\"demo.uraues_db.events_logs_agg7\")\n",
    "        \n",
    "        # Perform the merge operation using DataFrame APIs\n",
    "        merged_df = current_table.alias(\"target\").join(\n",
    "            batch_df.alias(\"source\"),\n",
    "            (col(\"target.Window_Start\") == col(\"source.Window_Start\")) &\n",
    "            (col(\"target.Window_End\") == col(\"source.Window_End\")) &\n",
    "            (col(\"target.VinNumber\") == col(\"source.VinNumber\")) &\n",
    "            (col(\"target.Parameter_Name\") == col(\"source.Parameter_Name\")),\n",
    "            \"full_outer\"\n",
    "        ).select(\n",
    "            coalesce(col(\"source.VinNumber\"), col(\"target.VinNumber\")).alias(\"VinNumber\"),\n",
    "            coalesce(col(\"source.Window_Start\"), col(\"target.Window_Start\")).alias(\"Window_Start\"),\n",
    "            coalesce(col(\"source.Window_End\"), col(\"target.Window_End\")).alias(\"Window_End\"),\n",
    "            coalesce(col(\"source.Parameter_Name\"), col(\"target.Parameter_Name\")).alias(\"Parameter_Name\"),\n",
    "            when(col(\"source.Count\").isNotNull() & col(\"target.Count\").isNotNull(),\n",
    "                 greatest(col(\"source.Count\"), col(\"target.Count\")))\n",
    "            .otherwise(coalesce(col(\"source.Count\"), col(\"target.Count\")))\n",
    "            .alias(\"Count\")\n",
    "        )\n",
    "        \n",
    "        # Log the merge results\n",
    "        print(\"Merged DataFrame:\")\n",
    "        merged_df.show(10, truncate=False)\n",
    "        print(f\"Merged DataFrame count: {merged_df.count()}\")\n",
    "        \n",
    "        # Write the merged data back to the Iceberg table\n",
    "        merged_df.write \\\n",
    "            .format(\"iceberg\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .saveAsTable(\"demo.uraues_db.events_logs_agg7\")\n",
    "        \n",
    "        print(f\"Batch {batch_id}: Merged {batch_df.count()} records into the Iceberg table.\")\n",
    "        \n",
    "        # Verify the write operation\n",
    "        updated_table = spark.table(\"demo.uraues_db.events_logs_agg7\")\n",
    "        print(\"Updated Iceberg table:\")\n",
    "        updated_table.show(10, truncate=False)\n",
    "        print(f\"Updated Iceberg table count: {updated_table.count()}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch {batch_id}: {str(e)}\")\n",
    "\n",
    "# Write the aggregated data to the Iceberg table using foreachBatch\n",
    "query = result_df.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .start()\n",
    "\n",
    "# Wait for the streaming query to terminate\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd322900-3dfa-416b-bc3c-aa93ba20a9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the aggregated data to the Iceberg table using merge\n",
    "def write_to_iceberg(batch_df, batch_id):\n",
    "    # Instead of creating a temporary view, use the DataFrame directly in the merge operation\n",
    "    batch_df.createOrReplaceTempView(\"updates\")\n",
    "    \n",
    "    spark.sql(\"\"\"\n",
    "    MERGE INTO demo.uraues_db.events_logs_aggregates2 AS target\n",
    "    USING (SELECT * FROM updates) AS source\n",
    "    ON target.id = source.id\n",
    "    WHEN MATCHED THEN\n",
    "      UPDATE SET\n",
    "        target.Count = source.Count\n",
    "    WHEN NOT MATCHED THEN\n",
    "      INSERT (id, VinNumber, Window_Start, Window_End, Parameter_Name, Count)\n",
    "      VALUES (source.id, source.VinNumber, source.Window_Start, source.Window_End, source.Parameter_Name, source.Count)\n",
    "    \"\"\")\n",
    "\n",
    "# Start the streaming query with foreachBatch\n",
    "query = result_df.writeStream \\\n",
    "    .foreachBatch(write_to_iceberg) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", \"s3://aws-emr-studio-381492251123-eu-central-1/stream_checkpoint/events_logs_aggregates2\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6322d586-dee0-4a4b-bf1e-e09661330329",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
