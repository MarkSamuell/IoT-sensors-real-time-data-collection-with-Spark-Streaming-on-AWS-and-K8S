{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-25T11:07:19.474057Z",
     "iopub.status.busy": "2024-08-25T11:07:19.473793Z",
     "iopub.status.idle": "2024-08-25T11:07:58.362021Z",
     "shell.execute_reply": "2024-08-25T11:07:58.361218Z",
     "shell.execute_reply.started": "2024-08-25T11:07:19.474028Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "066d3065f9384a21a587c4452efd0877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n<tbody><tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>2</td><td>application_1724580650220_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-3-203.eu-central-1.compute.internal:20888/proxy/application_1724580650220_0003/\" class=\"emr-proxy-link j-E0AWY1YJLDPG application_1724580650220_0003\" emr-resource=\"j-E0AWY1YJLDPG\n\" application-id=\"application_1724580650220_0003\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-3-241.eu-central-1.compute.internal:8042/node/containerlogs/container_1724580650220_0003_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></tbody></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import psycopg2\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, date_format, broadcast\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, MapType\n",
    "from elasticsearch import Elasticsearch, exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-25T11:07:58.363663Z",
     "iopub.status.busy": "2024-08-25T11:07:58.363456Z",
     "iopub.status.idle": "2024-08-25T11:07:58.419277Z",
     "shell.execute_reply": "2024-08-25T11:07:58.418687Z",
     "shell.execute_reply.started": "2024-08-25T11:07:58.363640Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b89ec301d6541f6bfd4ab617f34b52c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Elasticsearch configuration\n",
    "es_host = \"10.0.3.216\"\n",
    "es_port = 9200  # Ensure this is an integer\n",
    "es_scheme = \"http\"\n",
    "\n",
    "\n",
    "# Create Elasticsearch client\n",
    "es = Elasticsearch([{'host': es_host, 'port': es_port, 'scheme': es_scheme}])\n",
    "\n",
    "SEVs_mappings = {\n",
    "    \"properties\": {\n",
    "          \"SEV_ID\": {\n",
    "            \"type\": \"keyword\"\n",
    "          },\n",
    "          \"VIN\": {\n",
    "            \"type\": \"keyword\"\n",
    "          },\n",
    "          \"Timestamp\": {\n",
    "            \"type\": \"text\"  # Changed to text\n",
    "          },\n",
    "          \"Name\": {\n",
    "            \"type\": \"text\"\n",
    "          },\n",
    "          \"Type\": {\n",
    "            \"type\": \"text\"\n",
    "          },\n",
    "          \"Severity\": {\n",
    "            \"type\": \"keyword\"\n",
    "          },\n",
    "          \"NetworkType\": {\n",
    "            \"type\": \"keyword\"\n",
    "          },\n",
    "          \"NetworkID\": {\n",
    "            \"type\": \"keyword\"\n",
    "          },\n",
    "          \"SEV_Msg\": {\n",
    "            \"type\": \"text\"\n",
    "          },\n",
    "          \"Origin\": {\n",
    "            \"type\": \"keyword\"\n",
    "          },\n",
    "          \"IoC\": {\n",
    "            \"type\": \"object\"\n",
    "          }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-25T11:07:58.420697Z",
     "iopub.status.busy": "2024-08-25T11:07:58.420536Z",
     "iopub.status.idle": "2024-08-25T11:07:58.471462Z",
     "shell.execute_reply": "2024-08-25T11:07:58.470896Z",
     "shell.execute_reply.started": "2024-08-25T11:07:58.420677Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b2059b0ee3e4e15b4e978d100798add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if es.indices.exists(index=\"joined_raw_index\"):\n",
    "    es.indices.delete(index=\"joined_raw_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-25T11:07:58.472746Z",
     "iopub.status.busy": "2024-08-25T11:07:58.472586Z",
     "iopub.status.idle": "2024-08-25T11:08:05.773828Z",
     "shell.execute_reply": "2024-08-25T11:08:05.773197Z",
     "shell.execute_reply.started": "2024-08-25T11:07:58.472726Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0060c360a6fe412dacc473f320b26c56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|           Car Model|              VIN|\n",
      "+--------------------+-----------------+\n",
      "|Mercedes-Benz C-C...|5ZC7FP1CELU542056|\n",
      "|       Nissan Altima|THM35QS7GNW787602|\n",
      "|          Mazda CX-5|FCJBK973DHT960860|\n",
      "|Mercedes-Benz C-C...|FBKNVQPO3P8916909|\n",
      "|      Subaru Outback|3BV3J0SX765828596|\n",
      "|     Hyundai Elantra|KXAAA66Z49K588982|\n",
      "|        Toyota Camry|LKNZM5U782A364319|\n",
      "|      Subaru Outback|DLDMEC1HJTJ357517|\n",
      "|        Honda Accord|1TW2ZQDESAS678424|\n",
      "|        BMW 3 Series|DFUJL033LLU546604|\n",
      "|          Mazda CX-5|KBTBNV01AYK113631|\n",
      "|        BMW 3 Series|UKAR97F5FTE400236|\n",
      "|        Ford Mustang|PXH6QA41EYL676540|\n",
      "|       Nissan Altima|XPH52B0UNEB437989|\n",
      "|        Toyota Camry|6VBH7XD5361950961|\n",
      "|        Honda Accord|BCK7BFM186V025443|\n",
      "|        Toyota Camry|NGCDQU8TCBH013582|\n",
      "|     Hyundai Elantra|MYUMYC7WGGW452143|\n",
      "|       Nissan Altima|2DLXV2IWY9C152840|\n",
      "|    Chevrolet Malibu|WLT7C5ZYGJ2677014|\n",
      "+--------------------+-----------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaToElasticsearch\") \\\n",
    "    .config(\"spark.sql.streaming.checkpointLocation\", \"s3://aws-emr-studio-381492251123-eu-central-1/stream_checkpoint/checkpoint/\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# JDBC properties\n",
    "jdbc_url = \"jdbc:postgresql://10.0.3.216:5432/test_db\"\n",
    "jdbc_properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# Read static data from PostgreSQL\n",
    "car_model_df = spark.read.jdbc(url=jdbc_url, table=\"cars\", properties=jdbc_properties)\n",
    "\n",
    "# Rename column in the PostgreSQL DataFrame to match Kafka stream DataFrame\n",
    "car_model_df = car_model_df.withColumnRenamed(\"VIN Number\", \"VIN\")\n",
    "\n",
    "\n",
    "car_model_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-25T11:08:05.774876Z",
     "iopub.status.busy": "2024-08-25T11:08:05.774710Z",
     "iopub.status.idle": "2024-08-25T11:08:05.825450Z",
     "shell.execute_reply": "2024-08-25T11:08:05.824854Z",
     "shell.execute_reply.started": "2024-08-25T11:08:05.774855Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a45fe53a88a64bd5a5287a6abd0a12da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'sevs_index' already exists."
     ]
    }
   ],
   "source": [
    "def create_index_if_not_exists(es_client, index_name, mappings):\n",
    "    try:\n",
    "        if not es_client.indices.exists(index=index_name):\n",
    "            print(f\"Index '{index_name}' does not exist. Creating index...\")\n",
    "            es_client.indices.create(\n",
    "                index=index_name,\n",
    "                body={\n",
    "                    \"mappings\": mappings  # Use the provided mappings parameter\n",
    "                }\n",
    "            )\n",
    "            print(f\"Index '{index_name}' created successfully.\")\n",
    "        else:\n",
    "            print(f\"Index '{index_name}' already exists.\")\n",
    "    except exceptions.RequestError as e:\n",
    "        print(f\"RequestError: {e.info}\")\n",
    "    except exceptions.ConnectionError as e:\n",
    "        print(f\"ConnectionError: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating index: {e}\")\n",
    "\n",
    "# Create the index in Elasticsearch\n",
    "create_index_if_not_exists(es, \"sevs_index\", SEVs_mappings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-25T11:08:05.826460Z",
     "iopub.status.busy": "2024-08-25T11:08:05.826298Z",
     "iopub.status.idle": "2024-08-25T11:08:05.896357Z",
     "shell.execute_reply": "2024-08-25T11:08:05.895397Z",
     "shell.execute_reply.started": "2024-08-25T11:08:05.826440Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d8b3a76fcd043a6b1d15312536fe661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the schema\n",
    "# Define the schema for the IoC field as a MapType to accept any JSON\n",
    "ioc_schema = MapType(StringType(), StringType())\n",
    "\n",
    "# Define the full schema for the data\n",
    "schema = StructType([\n",
    "    StructField(\"SEV_ID\", StringType(), True),\n",
    "    StructField(\"VIN\", StringType(), True),\n",
    "    StructField(\"Timestamp\", TimestampType(), True),\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Type\", StringType(), True),\n",
    "    StructField(\"Severity\", StringType(), True),\n",
    "    StructField(\"NetworkType\", StringType(), True),\n",
    "    StructField(\"NetworkID\", StringType(), True),\n",
    "    StructField(\"SEV_Msg\", StringType(), True),\n",
    "    StructField(\"Origin\", StringType(), True),\n",
    "    StructField(\"IoC\", ioc_schema, True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-25T11:08:05.898064Z",
     "iopub.status.busy": "2024-08-25T11:08:05.897698Z",
     "iopub.status.idle": "2024-08-25T11:09:41.956719Z",
     "shell.execute_reply": "2024-08-25T11:09:41.955241Z",
     "shell.execute_reply.started": "2024-08-25T11:08:05.898028Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "086ab91fde584e3491f40af6d0e554a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6389b2d1a56466a82079e88abbcb484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "SparkStatementCancellationFailedException",
     "evalue": "Interrupted by user but Livy failed to cancel the Spark statement. The Livy session might have become unusable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/mnt/notebook-env/lib/python3.9/site-packages/sparkmagic/livyclientlib/command.py:61\u001b[0m, in \u001b[0;36mCommand.execute\u001b[0;34m(self, session)\u001b[0m\n\u001b[1;32m     60\u001b[0m     statement_id \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 61\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_statement_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatement_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/mnt/notebook-env/lib/python3.9/site-packages/sparkmagic/livyclientlib/command.py:134\u001b[0m, in \u001b[0;36mCommand._get_statement_output\u001b[0;34m(self, session, statement_id)\u001b[0m\n\u001b[1;32m    133\u001b[0m progress\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m=\u001b[39m statement\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprogress\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m)\n\u001b[0;32m--> 134\u001b[0m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m retries \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/mnt/notebook-env/lib/python3.9/site-packages/sparkmagic/livyclientlib/livysession.py:363\u001b[0m, in \u001b[0;36mLivySession.sleep\u001b[0;34m(self, retries)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msleep\u001b[39m(\u001b[38;5;28mself\u001b[39m, retries):\n\u001b[0;32m--> 363\u001b[0m     \u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_policy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseconds_to_sleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLivyClientTimeoutException\u001b[0m                Traceback (most recent call last)",
      "File \u001b[0;32m/mnt/notebook-env/lib/python3.9/site-packages/sparkmagic/livyclientlib/command.py:78\u001b[0m, in \u001b[0;36mCommand.execute\u001b[0;34m(self, session)\u001b[0m\n\u001b[1;32m     75\u001b[0m         response \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mhttp_client\u001b[38;5;241m.\u001b[39mcancel_statement(\n\u001b[1;32m     76\u001b[0m             session\u001b[38;5;241m.\u001b[39mid, statement_id\n\u001b[1;32m     77\u001b[0m         )\n\u001b[0;32m---> 78\u001b[0m         \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_for_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/notebook-env/lib/python3.9/site-packages/sparkmagic/livyclientlib/livysession.py:337\u001b[0m, in \u001b[0;36mLivySession.wait_for_idle\u001b[0;34m(self, seconds_to_wait)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39merror(error)\n\u001b[0;32m--> 337\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LivyClientTimeoutException(error)\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    340\u001b[0m     constants\u001b[38;5;241m.\u001b[39mYARN_RESOURCE_LIMIT_MSG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_info\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_printed_resource_warning\n\u001b[1;32m    342\u001b[0m ):\n",
      "\u001b[0;31mLivyClientTimeoutException\u001b[0m: Session 2 did not reach idle status in time. Current status is busy.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSparkStatementCancellationFailedException\u001b[0m Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspark\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfrom pyspark.sql.functions import from_json, col, date_format\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mfrom pyspark.sql.types import StructType, StructField, StringType, TimestampType, MapType\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mfrom pyspark.sql import SparkSession\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Initialize Spark session\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mspark = SparkSession.builder \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    .appName(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSEVsToElasticsearch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m) \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    .getOrCreate()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Kafka parameters\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mkafka_bootstrap_servers = \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mb-1.kafkaclustersevs.pgxp3x.c3.kafka.eu-central-1.amazonaws.com:9092,b-2.kafkaclustersevs.pgxp3x.c3.kafka.eu-central-1.amazonaws.com:9092\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mkafka_topic = \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtopic2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Create a DataFrame representing the stream of input lines from Kafka\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mkafka_df = spark.readStream \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    .format(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkafka\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m) \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    .option(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkafka.bootstrap.servers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, kafka_bootstrap_servers) \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    .option(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubscribe\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, kafka_topic) \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    .option(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstartingOffsets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mearliest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m) \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    .load()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Convert the value column from Kafka to a string\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mkafka_df = kafka_df.selectExpr(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCAST(value AS STRING)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Parse the JSON data using the schema\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mkafka_df = kafka_df.select(from_json(col(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m), schema).alias(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m))\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Flatten the DataFrame\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mkafka_df = kafka_df.select(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    col(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata.SEV_ID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m),\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    col(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata.VIN\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m),\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    date_format(col(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata.Timestamp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m), \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myyyy-MM-dd\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mT\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mHH:mm:ss.SSSSSS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m).alias(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTimestamp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m),\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    col(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata.Name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m),\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    col(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata.Type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m),\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    col(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata.Severity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m),\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    col(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata.NetworkType\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m),\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    col(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata.NetworkID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m),\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    col(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata.SEV_Msg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m),\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    col(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata.Origin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m),\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    col(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata.IoC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Broadcast the static DataFrame\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mbroadcast_car_model_df = broadcast(car_model_df)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Perform the join with the Kafka stream DataFrame\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mjoined_df = kafka_df.join(broadcast_car_model_df, kafka_df.VIN == broadcast_car_model_df.VIN, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mleft_outer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m) \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    .select(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        kafka_df.SEV_ID,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        kafka_df.VIN,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        kafka_df.Timestamp,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        kafka_df.Name,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        kafka_df.Type,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        kafka_df.Severity,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        kafka_df.NetworkType,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        kafka_df.NetworkID,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        kafka_df.SEV_Msg,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        kafka_df.Origin,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        kafka_df.IoC,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        broadcast_car_model_df[\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCar Model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m].alias(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCar Model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    )\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Elasticsearch configuration for Spark\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mes_write_conf = \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mes.nodes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: es_host,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mes.port\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: str(es_port),  # Convert port to string for Spark configuration\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mes.index.auto.create\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m  # Ensure that Spark does not attempt to auto-create the index\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m}\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Write the joined data to Elasticsearch\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mquery = joined_df.writeStream \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    .format(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.elasticsearch.spark.sql\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m) \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    .options(**es_write_conf) \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    .option(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mes.resource\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msevs_index\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m) \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    .outputMode(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m) \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    .start()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Await termination of the stream\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mquery.awaitTermination()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/notebook-env/lib/python3.9/site-packages/IPython/core/interactiveshell.py:2493\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2491\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2492\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2493\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2495\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2496\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2497\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/mnt/notebook-env/lib/python3.9/site-packages/sparkmagic/livyclientlib/exceptions.py:165\u001b[0m, in \u001b[0;36mwrap_unexpected_exceptions.<locals>.wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m conf\u001b[38;5;241m.\u001b[39mall_errors_are_fatal():\n",
      "File \u001b[0;32m/mnt/notebook-env/lib/python3.9/site-packages/sparkmagic/livyclientlib/exceptions.py:126\u001b[0m, in \u001b[0;36mhandle_expected_exceptions.<locals>.wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 126\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions_to_handle \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m conf\u001b[38;5;241m.\u001b[39mall_errors_are_fatal():\n",
      "File \u001b[0;32m/mnt/notebook-env/lib/python3.9/site-packages/sparkmagic/kernels/kernelmagics.py:483\u001b[0m, in \u001b[0;36mKernelMagics.spark\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m    479\u001b[0m args \u001b[38;5;241m=\u001b[39m parse_argstring_or_throw(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspark, line)\n\u001b[1;32m    481\u001b[0m coerce \u001b[38;5;241m=\u001b[39m get_coerce_value(args\u001b[38;5;241m.\u001b[39mcoerce)\n\u001b[0;32m--> 483\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_spark\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msamplemethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaxrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msamplefraction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoerce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/notebook-env/lib/python3.9/site-packages/sparkmagic/magics/sparkmagicsbase.py:129\u001b[0m, in \u001b[0;36mSparkMagicBase.execute_spark\u001b[0;34m(self, cell, output_var, samplemethod, maxrows, samplefraction, session_name, coerce, output_handler, cell_kind)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_spark\u001b[39m(\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    113\u001b[0m     cell,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m     cell_kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    122\u001b[0m ):\n\u001b[1;32m    123\u001b[0m     output_handler \u001b[38;5;241m=\u001b[39m output_handler \u001b[38;5;129;01mor\u001b[39;00m SparkOutputHandler(\n\u001b[1;32m    124\u001b[0m         html\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mipython_display\u001b[38;5;241m.\u001b[39mhtml,\n\u001b[1;32m    125\u001b[0m         text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mipython_display\u001b[38;5;241m.\u001b[39mwrite,\n\u001b[1;32m    126\u001b[0m         default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mipython_display\u001b[38;5;241m.\u001b[39mdisplay,\n\u001b[1;32m    127\u001b[0m     )\n\u001b[0;32m--> 129\u001b[0m     (success, out, mimetype) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspark_controller\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCommand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell_kind\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession_name\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m success:\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m conf\u001b[38;5;241m.\u001b[39mshutdown_session_on_spark_statement_errors():\n",
      "File \u001b[0;32m/mnt/notebook-env/lib/python3.9/site-packages/sparkmagic/livyclientlib/sparkcontroller.py:45\u001b[0m, in \u001b[0;36mSparkController.run_command\u001b[0;34m(self, command, client_name)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, client_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     44\u001b[0m     session_to_use \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_session_by_name_or_default(client_name)\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession_to_use\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/notebook-env/lib/python3.9/site-packages/sparkmagic/livyclientlib/command.py:80\u001b[0m, in \u001b[0;36mCommand.execute\u001b[0;34m(self, session)\u001b[0m\n\u001b[1;32m     78\u001b[0m         session\u001b[38;5;241m.\u001b[39mwait_for_idle()\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SparkStatementCancellationFailedException(\n\u001b[1;32m     81\u001b[0m         COMMAND_CANCELLATION_FAILED_MSG\n\u001b[1;32m     82\u001b[0m     )\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SparkStatementCancelledException(COMMAND_INTERRUPTED_MSG)\n",
      "\u001b[0;31mSparkStatementCancellationFailedException\u001b[0m: Interrupted by user but Livy failed to cancel the Spark statement. The Livy session might have become unusable."
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_json, col, date_format\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, MapType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SEVsToElasticsearch\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Kafka parameters\n",
    "kafka_bootstrap_servers = \"b-1.kafkaclustersevs.pgxp3x.c3.kafka.eu-central-1.amazonaws.com:9092,b-2.kafkaclustersevs.pgxp3x.c3.kafka.eu-central-1.amazonaws.com:9092\"\n",
    "kafka_topic = \"topic2\"\n",
    "\n",
    "# Create a DataFrame representing the stream of input lines from Kafka\n",
    "kafka_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Convert the value column from Kafka to a string\n",
    "kafka_df = kafka_df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "# Parse the JSON data using the schema\n",
    "kafka_df = kafka_df.select(from_json(col(\"value\"), schema).alias(\"data\"))\n",
    "\n",
    "# Flatten the DataFrame\n",
    "kafka_df = kafka_df.select(\n",
    "    col(\"data.SEV_ID\"),\n",
    "    col(\"data.VIN\"),\n",
    "    date_format(col(\"data.Timestamp\"), \"yyyy-MM-dd'T'HH:mm:ss.SSSSSS\").alias(\"Timestamp\"),\n",
    "    col(\"data.Name\"),\n",
    "    col(\"data.Type\"),\n",
    "    col(\"data.Severity\"),\n",
    "    col(\"data.NetworkType\"),\n",
    "    col(\"data.NetworkID\"),\n",
    "    col(\"data.SEV_Msg\"),\n",
    "    col(\"data.Origin\"),\n",
    "    col(\"data.IoC\")\n",
    ")\n",
    "\n",
    "# Broadcast the static DataFrame\n",
    "broadcast_car_model_df = broadcast(car_model_df)\n",
    "\n",
    "# Perform the join with the Kafka stream DataFrame\n",
    "joined_df = kafka_df.join(broadcast_car_model_df, kafka_df.VIN == broadcast_car_model_df.VIN, \"left_outer\") \\\n",
    "    .select(\n",
    "        kafka_df.SEV_ID,\n",
    "        kafka_df.VIN,\n",
    "        kafka_df.Timestamp,\n",
    "        kafka_df.Name,\n",
    "        kafka_df.Type,\n",
    "        kafka_df.Severity,\n",
    "        kafka_df.NetworkType,\n",
    "        kafka_df.NetworkID,\n",
    "        kafka_df.SEV_Msg,\n",
    "        kafka_df.Origin,\n",
    "        kafka_df.IoC,\n",
    "        broadcast_car_model_df[\"Car Model\"].alias(\"Car Model\")\n",
    "    )\n",
    "\n",
    "# Elasticsearch configuration for Spark\n",
    "es_write_conf = {\n",
    "    \"es.nodes\": es_host,\n",
    "    \"es.port\": str(es_port),  # Convert port to string for Spark configuration\n",
    "    \"es.index.auto.create\": \"true\"  # Ensure that Spark does not attempt to auto-create the index\n",
    "}\n",
    "\n",
    "# Write the joined data to Elasticsearch\n",
    "query = joined_df.writeStream \\\n",
    "    .format(\"org.elasticsearch.spark.sql\") \\\n",
    "    .options(**es_write_conf) \\\n",
    "    .option(\"es.resource\", \"sevs_index\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n",
    "# Await termination of the stream\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
